@article{DeVries2017,
abstract = {Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.},
archivePrefix = {arXiv},
arxivId = {1702.05538},
author = {{De Vries}, Terrance and Taylor, Graham W.},
eprint = {1702.05538},
file = {:Users/iserh/Mendeley Desktop/1702.05538.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--12},
title = {{Dataset augmentation in feature space}},
year = {2017}
}

@article{Garay-Maestre2019,
abstract = {Data augmentation is a widely considered technique to improve the performance of Convolutional Neural Networks during training. This step consists in synthetically generate new labeled data by perturbing the samples of the training set, which is expected to provide more robustness to the learning process. The problem is that the augmentation procedure has to be adjusted manually because the perturbations considered must make sense for the task at issue. In this paper we propose the use of Variational Auto-Encoders (VAEs) to generate new synthetic samples, instead of resorting to heuristic strategies. VAEs are powerful generative models that learn a parametric latent space of the input domain from which new samples can be generated. In our experiments over the well-known MNIST dataset, the data augmentation by VAEs improves the base results, yet to a lesser extent of that obtained by a well-adjusted conventional data augmentation. However, the combination of both conventional and VAE-guided data augmentations outperforms all the results, thereby demonstrating the goodness of our proposal.},
author = {Garay-Maestre, Unai and Gallego, Antonio Javier and Calvo-Zaragoza, Jorge},
doi = {10.1007/978-3-030-13469-3_4},
file = {:Users/iserh/Mendeley Desktop/CIARP-2018{\_}Data-Augmentation-VAEs.pdf:pdf},
isbn = {9783030134686},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional Neural Networks,Data augmentation,MNIST dataset,Variational auto-encoders},
number = {April 2020},
pages = {29--37},
title = {{Data augmentation via variational auto-encoders}},
volume = {11401 LNCS},
year = {2019}
}

@article{Jorge2018,
abstract = {Since the beginning of Neural Networks, different mechanisms have been required to provide a sufficient number of examples to avoid overfitting. Data augmentation, the most common one, is focused on the generation of new instances performing different distortions in the real samples. Usually, these transformations are problem-dependent, and they result in a synthetic set of, likely, unseen examples. In this work, we have studied a generative model, based on the paradigm of encoder-decoder, that works directly in the data space, that is, with images. This model encodes the input in a latent space where different transformations will be applied. After completing this, we can reconstruct the latent vectors to get new samples. We have analysed various procedures according to the distortions that we could carry out, as well as the effectiveness of this process to improve the accuracy of different classification systems. To do this, we could use both the latent space and the original space after reconstructing the altered version of these vectors. Our results have shown that using this pipeline (encoding-altering-decoding) helps the generalisation of the classifiers that have been selected.},
author = {Jorge, Javier and Vieco, Jes{\'{u}}s and Paredes, Roberto and Sanchez, Joan Andreu and Benedi, Jos{\'{e}} Miguel},
doi = {10.5220/0006618600960104},
file = {:Users/iserh/Mendeley Desktop/Empirical evaluation of variational autoencoders for data augmentation.pdf:pdf},
isbn = {9789897582905},
journal = {VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
keywords = {Data augmentation,Generative models,Variational autoencoder},
number = {Visigrapp},
pages = {96--104},
title = {{Empirical evaluation of variational autoencoders for data augmentation}},
volume = {5},
year = {2018}
}

@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Doersch, Carl},
eprint = {1606.05908},
file = {:Users/iserh/Mendeley Desktop/Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,prediction,structured,unsupervised learning,variational autoencoders},
pages = {1--23},
title = {{Tutorial on Variational Autoencoders}},
url = {http://arxiv.org/abs/1606.05908},
year = {2016}
}

@article{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
eprint = {1312.6114},
file = {:Users/iserh/Mendeley Desktop/Auto-Encoding Variational Bayes.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
number = {Ml},
pages = {1--14},
title = {{Auto-encoding variational bayes}},
year = {2014}
}

@article{Odaibo2019,
abstract = {In Bayesian machine learning, the posterior distribution is typically computationally intractable, hence variational inference is often required. In this approach, an evidence lower bound on the log likelihood of data is maximized during training. Variational Autoencoders (VAE) are one important example where variational inference is utilized. In this tutorial, we derive the variational lower bound loss function of the standard variational autoencoder. We do so in the instance of a gaussian latent prior and gaussian approximate posterior, under which assumptions the Kullback-Leibler term in the variational lower bound has a closed form solution. We derive essentially everything we use along the way; everything from Bayes' theorem to the Kullback-Leibler divergence.},
archivePrefix = {arXiv},
arxivId = {1907.08956},
author = {Odaibo, Stephen G.},
eprint = {1907.08956},
file = {:Users/iserh/Mendeley Desktop/1907.08956.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {1},
pages = {1--8},
title = {{Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function}},
year = {2019}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Moreno-Barea2020,
abstract = {Data augmentation (DA) is a key element in the success of Deep Learning (DL) models, as its use can lead to better prediction accuracy values when large size data sets are used. DA was not very much used with earlier neural network models before 2012, and the reason might be related to the type of models and the size of the data sets used. We investigate in this work, applying several state-of-the-art models based on Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), the effect of DA when using small size data sets, analyzing the results in terms of the prediction accuracy obtained according to the different characteristics of the training samples (number of instances and features, and class unbalance degree). We further introduce modifications to the standard methods used to generate the synthetic samples to alter the class balance representation, and the overall results indicate that with some computational effort a significant increase in prediction accuracy can be obtained when small data sets are considered.},
author = {Moreno-Barea, Francisco J. and Jerez, Jos{\'{e}} M. and Franco, Leonardo},
doi = {10.1016/j.eswa.2020.113696},
file = {:Users/iserh/Mendeley Desktop/A52-Moreno+Jerez+2020.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data augmentation,Deep Learning,GAN,Unbalanced sets,VAE},
pages = {113696},
publisher = {Elsevier Ltd},
title = {{Improving classification accuracy using data augmentation on small data sets}},
url = {https://doi.org/10.1016/j.eswa.2020.113696},
volume = {161},
year = {2020}
}

@article {lecun-98,
original = {orig/lecun-98.ps.gz},
author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
title = {Gradient-Based Learning Applied to Document Recognition},
journal = {Proceedings of the IEEE},
month = {11},
volume = {86},
number = {11},
pages = {2278-2324},
year = {1998}
}

@article{Higgins2017,
abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
journal = {Iclr 2017},
title = {{Β-Vae : Learning Basic Visual Concepts With a Constrained Variational Framework}},
year = {2017}
}

@misc{byerly2021branching,
      title={A Branching and Merging Convolutional Network with Homogeneous Filter Capsules}, 
      author={Adam Byerly and Tatiana Kalganova and Ian Dear},
      year={2021},
      eprint={2001.09136},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{cifar-orig,
author = {Krizhevsky, Alex},
year = {2012},
month = {05},
pages = {},
title = {Learning Multiple Layers of Features from Tiny Images},
journal = {University of Toronto}
}

@inproceedings{
foret2021sharpnessaware,
title={Sharpness-aware Minimization for Efficiently Improving Generalization},
author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=6Tm1mposlrM}
}

@inproceedings{liu2015faceattributes,
 title = {Deep Learning Face Attributes in the Wild},
 author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
 booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
 month = {12},
 year = {2015} 
}

@article{Sung2017,
abstract = {We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is de-signed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Be-sides providing improved performance on few-shot learn-ing, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective ap-proach for both of these two tasks.},
author = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H.S. and Hospedales, Timothy M.},
file = {:Users/iserh/Mendeley Desktop/Sung_Learning_to_Compare_CVPR_2018_paper.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1199--1208},
title = {{Learning to compare: Relation network for few-shot learning}},
year = {2017}
}

@misc{ladjal2019pcalike,
      title={A PCA-like Autoencoder}, 
      author={Saïd Ladjal and Alasdair Newson and Chi-Hieu Pham},
      year={2019},
      eprint={1904.01277},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{kl_divergence_close_form,
author = {Hershey, John and Olsen, Peder},
year = {2007},
month = {05},
pages = {IV-317 },
title = {Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models},
volume = {4},
isbn = {1-4244-0728-1},
journal = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol 4},
doi = {10.1109/ICASSP.2007.366913}
}

@article{LopezPinaya2019,
abstract = {The study of psychiatric and neurologic disorders typically involves the acquisition of a wide range of different types of data, such as brain images, electronic health records, and mobile phone sensors data. Each type of data has its unique temporal and spatial characteristics, and the process of extracting useful information from them can be very challenging. Autoencoders are neural networks that can automatically learn useful features and representations from the data; this makes them an ideal technique for simplifying the process of feature engineering in machine learning studies. In addition, autoencoders can be used for dimensionality reduction, denoising data, generative modeling, and even pretraining deep learning neural networks. In this chapter, we present the fundamental concepts of autoencoders and provide an overview of how they execute these tasks. Finally, we show some exemplary applications from brain disorders research.},
archivePrefix = {arXiv},
arxivId = {2003.05991},
author = {{Lopez Pinaya}, Walter Hugo and Vieira, Sandra and Garcia-Dias, Rafael and Mechelli, Andrea},
doi = {10.1016/B978-0-12-815739-8.00011-0},
eprint = {2003.05991},
file = {:Users/iserh/Mendeley Desktop/2003.05991.pdf:pdf},
isbn = {9780128157398},
journal = {Machine Learning: Methods and Applications to Brain Disorders},
keywords = {Autism,Autoencoder,Brain disorders,Denoising data,Depression,Dimension reduction,Electronic health records,Generative modeling,Pretraining,Representation learning},
pages = {193--208},
title = {{Autoencoders}},
year = {2019}
}

@article{Plumerault2020,
abstract = {Among the wide variety of image generative models, two models stand out: Variational Auto Encoders (VAE) and Generative Adversarial Networks (GAN). GANs can produce realistic images, but they suffer from mode collapse and do not provide simple ways to get the latent representation of an image. On the other hand, VAEs do not have these problems, but they often generate images less realistic than GANs. In this article, we explain that this lack of realism is partially due to a common underestimation of the natural image manifold dimensionality. To solve this issue we introduce a new framework that combines VAE and GAN in a novel and complementary way to produce an auto-encoding model that keeps VAEs properties while generating images of GAN-quality. We evaluate our approach both qualitatively and quantitatively on five image datasets.},
archivePrefix = {arXiv},
arxivId = {2012.11551},
author = {Plumerault, Antoine and Borgne, Herv{\'{e}} Le and Hudelot, C{\'{e}}line},
eprint = {2012.11551},
file = {:Users/iserh/Mendeley Desktop/2012.11551.pdf:pdf},
number = {January},
pages = {1--16},
title = {{AVAE: Adversarial Variational Auto Encoder}},
url = {http://arxiv.org/abs/2012.11551},
year = {2020}
}

@article{Larsen2016,
abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
archivePrefix = {arXiv},
arxivId = {1512.09300},
author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
eprint = {1512.09300},
file = {:Users/iserh/Mendeley Desktop/Autoencoding beyond pixels using a learned similarity metric.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {2341--2349},
title = {{Autoencoding beyond pixels using a learned similarity metric}},
volume = {4},
year = {2016}
}

@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Klys2018,
abstract = {Variational autoencoders (VAEs) [10, 20] are widely used deep generative models capable of learning unsupervised latent representations of data. Such representations are often difficult to interpret or control. We consider the problem of unsupervised learning of features correlated to specific labels in a dataset. We propose a VAE-based generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret. Our model, the Conditional Subspace VAE (CSVAE), uses mutual information minimization to learn a low-dimensional latent subspace associated with each label that can easily be inspected and independently manipulated. We demonstrate the utility of the learned representations for attribute manipulation tasks on both the Toronto Face [23] and CelebA [15] datasets.},
archivePrefix = {arXiv},
arxivId = {1812.06190},
author = {Klys, Jack and Snell, Jake and Zemel, Richard},
eprint = {1812.06190},
file = {:Users/iserh/Mendeley Desktop/1812.06190.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {6444--6454},
title = {{Learning latent subspaces in variational autoencoders}},
volume = {2018-December},
year = {2018}
}

@article{Antoniou2017,
archivePrefix = {arXiv},
arxivId = {1711.04340},
author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
eprint = {1711.04340},
file = {:Users/iserh/Mendeley Desktop/Data Augmentation with Improved Generative Adversarial Networks.pdf:pdf},
journal = {arXiv},
pages = {1--14},
title = {{Data augmentation generative adversarial networks}},
year = {2017}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Senior2020,
abstract = {Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.},
author = {Senior, Andrew W and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v{Z}}{\'{i}}dek, Augustin and Nelson, Alexander W R and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/s41586-019-1923-7},
issn = {1476-4687},
journal = {Nature},
number = {7792},
pages = {706--710},
title = {{Improved protein structure prediction using potentials from deep learning}},
url = {https://doi.org/10.1038/s41586-019-1923-7},
volume = {577},
year = {2020}
}
